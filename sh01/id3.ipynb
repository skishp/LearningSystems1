{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3 - Algorithm \n",
    "In this notebook we want to have a close look at the ID3 Algorithm. To do so we first import all necessary libraries which we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import math\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ID3 Algorithm describes a Decision-Tree we first want to implement a Node Class. \n",
    "The Class Node needs the attributes\n",
    " - children\n",
    " - value\n",
    " - isLeaf\n",
    " - pred\n",
    "The attribute children will be initialized as a List. Obviously every node needs a value. With isLeaf we want to check whether the node is a Leaf or not. Leaf is basically the end nodes of a Tree. Last we give our node an attribute pred where we store the prediction result of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.children = []\n",
    "        self.value = \"\"\n",
    "        self.isLeaf = False\n",
    "        self.pred = \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented our Class Node, we have to think about the decision rules we want to implement. In the case of the ID3 Algorithm we need two function called `entropy` and `info_gain`. The `entropy` function will be used to calculate the uncertainty of the data set. The `info_gain` will calculate the information gain. The information gain is a measure of the difference in entropy from before to after the Data Set is split on a specific attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(examples):\n",
    "    \"\"\"\n",
    "    Compute the entropy of a set of examples\n",
    "    :param examples: set of examples as pandas set\n",
    "    :return: entropy as float\n",
    "    \"\"\"\n",
    "    pos = 0.0\n",
    "    neg = 0.0\n",
    "    for _, row in examples.iterrows():\n",
    "        # compute total number of positive and\n",
    "        # negative samples\n",
    "        if row[\"answer\"] == \"yes\":\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    if pos == 0.0 or neg == 0.0:\n",
    "        # entropy is zero if there is no data\n",
    "        return 0.0\n",
    "    else:\n",
    "        p = pos / (pos + neg) # p(+)\n",
    "        n = neg / (pos + neg) # p(-)\n",
    "        # entropy is\n",
    "        # h = -(p(+) * log(p(+) + p(-) * log(p(-)))\n",
    "        return -(p * math.log(p, 2) + n * math.log(n, 2))\n",
    "\n",
    "def info_gain(examples, attr):\n",
    "    \"\"\"\n",
    "    Compute the information gain given a set of examples and attributes. This element is\n",
    "    used to split the tree.\n",
    "    :param examples: pandas frame of examples\n",
    "    :param attr:  attributes\n",
    "    :return: information gain as float\n",
    "    \"\"\"\n",
    "    uniq = np.unique(examples[attr])\n",
    "    # info gain is G = H(S) - \\_sum_a H(S|A=a) for data S and attributes a \\in A\n",
    "    # so we first find the entropy of S\n",
    "    gain = entropy(examples)\n",
    "    for u in uniq:\n",
    "        subdata = examples[examples[attr] == u]\n",
    "        sub_e = entropy(subdata)\n",
    "        # and then subtract the conditional entropy H(S|A=a)\n",
    "        # normalized by N_a/N_total\n",
    "        gain -= (float(len(subdata)) / float(len(examples))) * sub_e\n",
    "    return gain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented our `info_gain` function we need a function `find_max_feat`. This function shall iterate over all the attributes our Data Set offers and find the attribute with the highest information gain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_feat(examples, attrs):\n",
    "    \"\"\"\n",
    "    Find feature with maximum information gain\n",
    "    :param examples: pandas frame of examples\n",
    "    :param attrs: attributes\n",
    "    :return: feature with max. info gain as str\n",
    "    \"\"\"\n",
    "\n",
    "    max_gain = -1e-10\n",
    "    max_feat = \"\"\n",
    "    print(\"Remaining examples: \",examples.shape)\n",
    "    print(\"Remaining attributes: \",attrs)\n",
    "    for feature in attrs:\n",
    "        gain = info_gain(examples, feature)\n",
    "        print(\"feature: \",feature)\n",
    "        print(\"gain: \", gain)\n",
    "        if gain > max_gain:\n",
    "            max_gain = gain\n",
    "            max_feat = feature\n",
    "    return max_feat, max_gain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we want to implement the ID3 Algorithm. The basic idea of the algorithm is summarized as the following\n",
    "- Calculate the entropy of every attribute a of the Data set \n",
    "- Split the Data Set into subsets using the attribute for which the resulting entropy after splitting the information gain is maximized. \n",
    "- Make decision tree node containing the attribute\n",
    "- Recurse on subsets using the remaining attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID3(examples, attrs, run=0):\n",
    "    \"\"\"\n",
    "    Recursively construct a decision tree using the ID3 Algorithm\n",
    "    :param examples: a set of examples\n",
    "    :param attrs: attributes\n",
    "    :return: returns the root node of the decision tree\n",
    "    \"\"\"\n",
    "    root = Node()\n",
    "\n",
    "    max_feat, max_gain = find_max_feat(examples, attrs)\n",
    "    root.value = max_feat\n",
    "\n",
    "    print (\"Iteration Depth %d: Feature\" %run, max_feat, \"has highest information gain with approx. %.3f bits\" % max_gain)\n",
    "\n",
    "\n",
    " # get list of attributes of max_feat\n",
    "    uniq = np.unique(examples[max_feat])\n",
    "\n",
    "    for u in uniq:\n",
    "        # find all samples with this particular attribute of max_feat\n",
    "        subdata = examples[examples[max_feat] == u]\n",
    "        # check entropy of subdata\n",
    "        entropy_sub = entropy(subdata)\n",
    "        if entropy_sub == 0.0 or len(attrs) == 1:\n",
    "            # if it's zero, create new node as leaf\n",
    "            # and set answer accordingly\n",
    "            newNode = Node()\n",
    "            pos = 0\n",
    "            neg = 0\n",
    "            for _, row in subdata.iterrows():\n",
    "                # compute total number of positive and\n",
    "                # negative samples\n",
    "                if row[\"answer\"] == \"yes\":\n",
    "                    pos += 1\n",
    "                else:\n",
    "                    neg += 1\n",
    "            if( pos > neg ) :\n",
    "                leaf_answer = \"yes\"\n",
    "            else :\n",
    "                leaf_answer = \"no\"\n",
    "            #leaf_answer =  np.unique(subdata[\"answer\"])\n",
    "            newNode.isLeaf = True\n",
    "            newNode.value = u\n",
    "            newNode.pred = leaf_answer\n",
    "            root.children.append(newNode)\n",
    "            print(\"Entropy is zero. Created leaf node with answer %s for value %s\" % (leaf_answer[0], u))\n",
    "        else:\n",
    "            # if it's not zero, then we are not done yet\n",
    "            # create new node, set value and attributes\n",
    "            dummyNode = Node()\n",
    "            dummyNode.value = u\n",
    "            new_attrs = attrs.copy()\n",
    "            new_attrs.remove(max_feat)\n",
    "            print(\"Entropy is %.3f. Created inner node for value %s\" % (entropy_sub, u))\n",
    "            # call ID3 recursively\n",
    "            child = ID3(subdata, new_attrs, run+1)\n",
    "            dummyNode.children.append(child)\n",
    "            root.children.append(dummyNode)\n",
    "    return root\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the tree we need a function `printTree`. Also we have to implement a function `predict`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTree(root: Node, depth=0):\n",
    "    for i in range(depth):\n",
    "        print(\"\\t\", end=\"\")\n",
    "    print(root.value, end=\"\")\n",
    "    if root.isLeaf:\n",
    "        print(\" -> \", root.pred)\n",
    "    print()\n",
    "    for child in root.children:\n",
    "        printTree(child, depth + 1)\n",
    "\n",
    "def predict(current_node, x):\n",
    "    if current_node.isLeaf:\n",
    "        return current_node.pred\n",
    "    else:\n",
    "        for child in current_node.children:\n",
    "            if x[current_node.value] == child.value:\n",
    "                if len(child.children):\n",
    "                    return predict(child.children[0], x)\n",
    "                else:\n",
    "                    return child.pred\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally load in our data and try the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the following data set:\n",
      "     sky airtemp humidity    wind water forecast answer\n",
      "0  sunny    warm   normal  strong  warm     same    yes\n",
      "1  sunny    warm     high  strong  warm     same    yes\n",
      "2  rainy    cold     high  strong  warm   change     no\n",
      "3  sunny    warm     high  strong  cool   change    yes\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"EnjoySport.csv\").applymap(str).applymap(str.strip)\n",
    "print(\"Loaded the following data set:\")\n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to initialize a list of all the attributes, but without the answer attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with the following features:\n",
      "['sky', 'airtemp', 'humidity', 'wind', 'water', 'forecast']\n"
     ]
    }
   ],
   "source": [
    "features = [feat for feat in data]\n",
    "features.remove(\"answer\")\n",
    "print(\"with the following features:\")\n",
    "print(features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialize the ID3 tree and print it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining examples:  (4, 7)\n",
      "Remaining attributes:  ['sky', 'airtemp', 'humidity', 'wind', 'water', 'forecast']\n",
      "feature:  sky\n",
      "gain:  0.8112781244591328\n",
      "feature:  airtemp\n",
      "gain:  0.8112781244591328\n",
      "feature:  humidity\n",
      "gain:  0.12255624891826566\n",
      "feature:  wind\n",
      "gain:  0.0\n",
      "feature:  water\n",
      "gain:  0.12255624891826566\n",
      "feature:  forecast\n",
      "gain:  0.31127812445913283\n",
      "Iteration Depth 0: Feature sky has highest information gain with approx. 0.811 bits\n",
      "Entropy is zero. Created leaf node with answer n for value rainy\n",
      "Entropy is zero. Created leaf node with answer y for value sunny\n",
      "done creating tree!\n",
      "\n",
      "\n",
      "Final Tree:\n",
      "\n",
      "sky\n",
      "\trainy ->  no\n",
      "\n",
      "\tsunny ->  yes\n",
      "\n",
      "each row is a level of the tree and -> indicate leaf nodes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "root = ID3(data, features)\n",
    "print(\"done creating tree!\\n\\n\")\n",
    "print(\"Final Tree:\\n\")\n",
    "printTree(root)\n",
    "print(\"each row is a level of the tree and -> indicate leaf nodes\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last we want to make the prediction and print it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction for \n",
      " sky          sunny\n",
      "airtemp       warm\n",
      "humidity    normal\n",
      "wind        strong\n",
      "water         warm\n",
      "forecast      same  is  yes\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "x = data.iloc[0,:-1]\n",
    "print(\"prediction for \\n\", x.to_string(), \" is \", predict(root, x) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
